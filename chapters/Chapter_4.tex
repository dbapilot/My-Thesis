%\chapter{Implementaion}
\begin{center}
    --Running--
\end{center}
\section{Implementations}
This section describes a step-by-step process for effectively deploying materialized views, and PSO algorithm based on the earlier theoretical foundations, to reduce query execution time, minimize computational overhead, and enhance overall performance. The implementation begins with an overview of the tools and technologies utilized in this project, providing essential context before delving into the optimization process.

\subsection{Used Software and Tools}
This project uses some software tools and technologies related to database management, query optimization, and technical documentation. Collectively, these tools support database management, query optimization, data analysis, and technical documentation throughout the research and development process.

\begin{enumerate}[label=(\roman*)]
\item\textbf{SQL Server Management Studio :} SQL Server Management Studio(SSMS) is actually an integrated environment that can maintain the SQL Server infrastructure. It is used to access, manage, configure, administer, and develop all components of SQL Server. In addition,t is used to manage the schema, tables, access to SQL Server, and materialized views of the database. Also, it monitors the performance of the query using execution plans and statistics, and it helps to debug SQL queries and optimize their execution. Here MSSQL is used to host the database, create tables, and define materialized views, enabling query execution and performance measurement.


\item\textbf{{Visual Studio Code:}} Microsoft created this open-source integrated development environment (IDE) for web browsers, Linux, macOS, and Windows. It is used to write Python scripts for automation (e.g., measuring query performance), integrate MSSQL queries with Python using extensions, and debug Python and SQL scripts. VS Code is used as the primary Integrated Development Environment (IDE) for writing, debugging, and running Python scripts that implement the PSO algorithm and interact with the SQL Server database.

\item\textbf{Overleaf:} Overleaf is an open-source online, real-time collaborative LaTeX\footnote{LaTeX is a powerful typesetting system commonly used for academic and technical documents. It includes features designed for the production of technical and scientific documentation.} editor that simplifies the process of creating, editing, and collaborating on LaTeX documents. The whole project is written with the help of Overleaf.

\item\textbf{Microsoft SQL Server:} Microsoft SQL Server is a relational database that provides a wide range of features for storing, processing, and securing data. SQL Server hosts the database, stores the tables and materialized views, and executes the SQL queries, allowing the measurement and optimization of query performance using the PSO algorithm.


\item\textbf{Python:} Python is a high-level interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and released in 1991 \cite{martin2023stam,wijanarko2020prediksi} . It has become one of the most popular programming languages worldwide. The structure of the language and its object-oriented approach help programmers to write logical and clear code for small and large projects. Python libraries (packages) effectively simplify many important processes such as analysing and visualizing data, retrieving unstructured data from the web, image processing, building machine learning models, and textual information \cite{Samira_Gholizadeh2022}. Here, it has been used to implement the PSO algorithm to optimize query performance, connects to the database using libraries like pyodbc, measures, executes, and analyzes the database programmatically.

\item\textbf{pyodbc:} Pyodbc is an open-source Python module that makes accessing ODBC\footnote{Open Database Connectivity (ODBC) is a standardized application programming interface (API) for accessing databases.} databases simple. It implements the DB API\footnote{An Application Programming Interface (API) is a set of protocols, tools, and definitions that allow different software applications to communicate with each other.} 2.0 specification but is packed with even more Pythonic convenience. The pyodbc library connects Python to MSSQL, allowing programmatic execution of SQL queries and retrieval of results.

\item\textbf{pandas:} The pandas constitute an open-source data manipulation and analysis tool that is fast, powerful, flexible, and easy to use. It provides data structures like DataFrames and Series, which are particularly useful for working with structured data. It is built entirely on the Python programming language. Pandas is used to analyze query performance data, such as execution times and storage costs, for better insights.

\item\textbf{matplotlib:} Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. \enquote{Matplotlib makes easy things easy and hard things possible} \cite{matplotlib}. It has been used to visualize query performance metrics and PSO convergence, making results easier to interpret.

\item\textbf{Git:} Git is a version control system that is used for tracking the changes made in the files and for enabling a collaborative software world. Git is incredibly popular due to its flexibility, speed, and ability to support almost any workflow. It has become the go-to standard for version control, with around 90 per cent of developers worldwide using it as their primary system. Git is used to track changes in Python scripts, LaTeX documents, and SQL files, making it an essential tool for collaboration and version control during thesis development.

\end{enumerate}

\clearpage

\subsection{Practical Implementation }
 \begin{center}
     \textbf{..... Running ...........}
 \end{center}
\subsubsection{Database Creation}
Choosing a database or creating one if not available is the initial step of the implementation. For the demonstration purpose, one database called \texttt{AccessAuditDB} is created. The database was populated with tables and sample data to simulate a real-world environment. Relationships between tables are defined using primary keys and foreign keys to ensure data integrity and enable efficient querying, enabling the testing and optimization of queries using materialized views and the PSO algorithm.

The following SQL query checks if the database \texttt{AccessAuditDB} exists and creates it if it does not: \vspace{.4cm}

\input{SQL/Create_database}

\subsubsection{Identify Complex Queries} This is a very important part of identifying the frequent queries that need to be optimized. It refers to the process of analyzing a database workload to pinpoint queries that are resource-intensive, frequently executed, or critical to performance. SQL Server Management Studio or direct SQL query can be used to analyze execution logs and identify the frequently executed and resource-intensive queries. Also identify frequently used sub-queries, aggregation (Queries with GROUP BY, SUM, AVG)s, or frequently joins between tables. For example, the following query to identify long-running queries: \vspace{.4cm}

\input{SQL/Query_identify}

The above SQL query retrieves the top 10 most time-consuming queries from SQL Server by analyzing the \(\texttt{sys.dm\_exec\_query\_stats}\) Dynamic Management Views(DMV)\footnote{DMVs are system defined views for database administrators and developers to troubleshoot performance issues, identify missing indexes, analyze query performance, and monitor resource usage efficiently.}. It calculates the total elapsed time in milliseconds, counts the number of executions, and fetches the query text using \(\texttt{sys.dm\_exec\_sql\_text}\). The results are sorted by \(\texttt{total\_elapsed\_time}\) in descending order to highlight the slowest queries for performance tuning and optimization. These queries were then used to create materialized views, which were optimized using the PSO algorithm to improve overall query performance.

\subsubsection{ Materialized views Creation (Indexed Views):}\label{Query_decomposition} Once the top slow queries are sorted out, MVs(In MSSQL Server Mvs are implemented as Indexed views ) are created for each of these queries to store their precomputed results.\vspace{.4cm}

  %\input{SQL/Create_MSSQL}
  \input{SQL/Create_Views}\vspace{.4cm}

  \subsubsection{View Maintenance and Refresh Strategies:}\label{View_maintainance} Manual or automatic refresh strategies can be set up according to the requirements query to create a scheduled job: \vspace{.4cm}

\input{SQL/Refresh_config}


\subsection{Automation with the PSO algorithm:} \label{Cost_evaluation}
 The following Python code illustrates the implementation of the PSO algorithm for multiple values in the database system. The goal is to minimize query execution time and CPU cost by selecting the most beneficial views to materialize. The following sections provide a detailed explanation of each function in the implementation.



%\section*{\textbf{Code example to selecting the optimal view using PSO algorithm.} \vspace{.4cm}}

\input{Python/PSO_python}\vspace{.4cm}

A sequence diagram is included to provide a step-by-step visualisation of how this code operates, making it more understandable.
  
 % \input{Python/PSO_Best_Response} \vspace{.4cm}


\clearpage


% Define block styles


% Inserting the image
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figure/seq.diagram .png} % Replace 'example-image' with your image file name
    \caption{Sequence diagram of code}
    \label{fig:sample_image}
\end{figure}






\subsubsection{Output from PSO automation }  Upon execution, the algorithm produced the following output: \vspace{.4cm}



  \input{Python/Python_output} \vspace{.4cm}
  

  %\input{Python/python_output2} \vspace{.4cm}





 For each particle, the PSO algorithm evaluates the CPU cost and execution times, updates personal and global best, and updates velocity and position. Finally, it prints the optimal combination of materialized view and the corresponding cost and time.\vspace{.4cm}
  

\textbf{For the full code and further test cases, please refer to the Appendix~\ref{fullcode:Fullcode}.}


\subsection{Performance Testing and Data Analysis} After creating the materialized views, performance metrics were collected using SQL query statistics. Query execution times and CPU usages are recorded using built-in database monitoring tools, such as Dynamic Management Views (DMVs) or with the help of SQL queries like. 

 \input{SQL/Query_statistics}

\begin{enumerate}


    \item \textbf{ Query response time comparison:}\\
The tests were performed using \textit{MSSQL} on a system running \textit{Windows 10 Pro} with the following specifications:
\begin{itemize}
    \item \textbf{System Type}: 64-bit operating system, x64-based processor.
    \item \textbf{Processor}: 12th Gen Intel(R) Core(TM) i7-1255U, 1.70 GHz.
    \item \textbf{RAM}: 32 GB.
\end{itemize}\vspace{.4cm}
After executing each query several times, both with and without materialized views, the \textit{response time}\footnote{Time taken to execute queries} and \textit{CPU usage}\footnote{Percentage of CPU utilization during query execution and refresh processes.} were noted in the "Messages" tab in SSMS. The effectiveness of the materialized views was also evaluated using a comparison table and the percentage difference formula.

\begin{itemize}
\item\textbf{Differences between optimization methods:} The \hyperref[comparison_table]{table~\ref*{comparison_table}} presents a performance comparison of query execution times using three different approaches: \textit{Direct SELECT Aggregation}, \textit{Indexed View }, and \textit{Materialized View}. The results show that the MV approach consistently outperforms the other methods, with an average execution time of \textbf{5 ms}, compared to \textbf{14.8 ms} for indexed View query and \textbf{66.6 ms} for direct \textit{SELECT} aggregation. This demonstrates that materialized views significantly reduce query execution times, providing a \textbf{92.5\%} improvement over direct aggregation and a \textbf{66.2\%} improvement over indexed views. The consistent performance across multiple runs highlights the reliability and efficiency of materialized views in optimizing database queries.\vspace{.4cm}
 
 \input{Table/Response_time_comparion}\vspace{.4cm
 }

 \item\textbf{Analysis of percentage differences in query performance:}
 
\input{Math/Differences}

\begin{comment}\[
\text{Difference (\%)} = \frac{\text{Without MV} - \text{With MV}}{\text{Without MV}} \times 100 = \frac{2.35 - 0.45}{2.35} \times 100 \approx 80.85\%
\]
\end{comment}

The output indicates an 80.85\% improvement in performance. Here, \( W = 2.35 \) represents the initial execution time (without optimization), and \( M = 0.45 \) represents the improved execution time (with optimization). This significant reduction in execution time demonstrates the effectiveness of the optimization technique, as it reduces the query processing time by approximately 80.85\%, leading to faster and more efficient database operations.\vspace{.4cm}

\item \textbf{Impact of MV on query execution time:} The performance of queries with and without materialized views (MV) is compared in \hyperref[tab:performance]{Table~\ref*{tab:performance}}. The table shows the execution times for three queries without materialized views and with materialized views, along with the percentage difference in performance. As evident from the results, the use of materialized views significantly reduces query execution time, with improvements ranging from **79.34\%** to **83.61\%**.

\begin{table}[h!]
    \centering
    \caption{Performance Comparison}
    \label{tab:performance}
    \rowcolors{2}{gray!10}{white} % Alternate row colors
    \begin{tabular}{lccc}
        \toprule
        \rowcolor{blue!10} % Header row color
        \textbf{Query} & \textbf{Without MV (s)} & \textbf{With MV (s)} & \textbf{Difference (\%)} \\
        \midrule
        Query 1 & 2.35 & 0.45 & \cellcolor{white!20}80.85 \\
        Query 2 & 3.78 & 0.62 & \cellcolor{gray!10}83.61 \\
        Query 3 & 4.21 & 0.87 & \cellcolor{white!20}79.34 \\
        \bottomrule
    \end{tabular}
\end{table}

%\item \textbf{Bar Chart Comparison:} As shown in the \hyperref[fig:execution-plan]{barchart}, the execution time demonstrates the optimization achieved by using materialized views. Direct select aggregation is the least efficient as it involves computing aggregation directly from the raw data during query execution, which is resource-intensive and time-consuming. On the other hand, a materialized view is the fastest one. Although the choice of method depends on the trade-off between query speed, storage requirements, and maintenance.



%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{Figure/Bar_chart.png} % Replace with your image file name
%\caption{Comparison of query execution time} % Caption for the screenshot
%\label{fig:execution-plan} % Label for referencing
%\end{figure}


\end{itemize} \vspace{.4cm}

    \item \textbf{CPU performance analysis:}

The following \hyperref[fig:cpu_usage]{bar chart} illustrates the average CPU usage for query execution before and after materialized views (MVs) are implemented. The x-axis is used to represent two conditions: \textit{Before} MVs (without materialized views) and \textit{After} MVs (with materialized views). The y-axis is used to represent the average CPU usage as a percentage. As it is evident from the graph, the CPU utilization average went down significantly from \textbf{85\%} (before MVs) to \textbf{30\%} (after MVs), showcasing the effectiveness of materialized views in reducing CPU burden in query processing. The reduction can be accounted for due to precomputation and caching of query output, which reduces real-time computation and subsequently CPU consumption.

%\input{Table_chart/cpu_usage}

\begin{figure}[h!] % Use the figure environment
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        ybar, 
        symbolic x coords={Before MV, After MV}, % Labels for x-axis
        xtick=data, 
        ylabel={CPU Usage (\%)}, % Y-axis Label (updated to CPU Usage)
        xlabel={Query Execution Type}, % X-axis Label
        nodes near coords, % Shows values above bars
        bar width=25pt, % Adjusted bar width
        enlarge x limits=0.5, % Spacing between bars
        ymin=0, ymax=100, % Y-axis limits
        area legend, % Enables legend area
        legend style={
            at={(0.5,-0.25)}, % Position of the legend (below the chart)
            anchor=north, % Anchors the legend to the north (top) of the point
            yshift=-5pt, % Adds a small gap between xlabel and legend
            font=\small, % Smaller font for legend
        }, 
        bar shift=0pt, % Aligns bars properly
        ymajorgrids=true, % Adds horizontal grid lines
        grid style={dashed, gray!30}, % Grid line style
        ytick={0, 20, 40, 60, 80, 100}, % Custom Y-axis ticks
        xticklabel style={font=\small}, % Smaller font for x-axis labels
        yticklabel style={font=\small}, % Smaller font for y-axis labels
        axis on top, % Places axis lines on top of the bars
    ]
    \addplot[
        fill=red!30, % First bar fill color (red)
        draw=red, % First bar border color (red)
    ] coordinates {(Before MV, 85)(After MV, 30)}; % First bar value (Before MV)
   % \addlegendentry{Before MV} % Legend entry for the first bar

    \addplot[
        fill=green!30, % Second bar fill color (green)
        draw=green, % Second bar border color (green)
    ] coordinates {(After MV, 30)(After MV, 30)}; % Second bar value (After MV)
    %\addlegendentry{After MV} % Legend entry for the second bar

    \end{axis}
\end{tikzpicture}
\caption{Comparison of CPU Usage}
\label{fig:cpu_usage}
\end{center}
\end{figure}
\end{enumerate}


 \textbf{Data will be updated once the final implementation is done}.

