%\chapter{Implementaion}
\begin{center}
    --Running--
\end{center}
\section{Implementations}
This section describes a step-by-step process for effectively deploying materialized views, and PSO algorithm based on the earlier theoretical foundations, to reduce query execution time, minimize computational overhead, and enhance overall performance. The implementation begins with an overview of the tools and technologies utilized in this project, providing essential context before delving into the optimization process.

\subsection{Used Software and Tools}
This project uses some software tools and technologies related to database management, query optimization, and technical documentation. Collectively, these tools support database management, query optimization, data analysis, and technical documentation throughout the research and development process.

\begin{enumerate}[label=(\roman*)]
\item\textbf{SQL Server Management Studio :} SQL Server Management Studio(SSMS) is actually an integrated environment that can maintain the SQL Server infrastructure. It is used to access, manage, configure, administer, and develop all components of SQL Server. In addition,t is used to manage the schema, tables, access to SQL Server, and materialized views of the database. Also, it monitors the performance of the query using execution plans and statistics, and it helps to debug SQL queries and optimize their execution. Here MSSQL is used to host the database, create tables, and define materialized views, enabling query execution and performance measurement.


\item\textbf{{Visual Studio Code:}} Microsoft created this open-source integrated development environment (IDE) for web browsers, Linux, macOS, and Windows. It is used to write Python scripts for automation (e.g., measuring query performance), integrate MSSQL queries with Python using extensions, and debug Python and SQL scripts. VS Code is used as the primary Integrated Development Environment (IDE) for writing, debugging, and running Python scripts that implement the PSO algorithm and interact with the SQL Server database.

\item\textbf{Overleaf:} Overleaf is an open-source online, real-time collaborative LaTeX\footnote{LaTeX is a powerful typesetting system commonly used for academic and technical documents. It includes features designed for the production of technical and scientific documentation.} editor that simplifies the process of creating, editing, and collaborating on LaTeX documents. The whole project is written with the help of Overleaf.

\item\textbf{Microsoft SQL Server:} Microsoft SQL Server is a relational database that provides a wide range of features for storing, processing, and securing data. SQL Server hosts the database, stores the tables and materialized views, and executes the SQL queries, allowing the measurement and optimization of query performance using the PSO algorithm.


\item\textbf{Python:} Python is a high-level interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and released in 1991 \cite{martin2023stam,wijanarko2020prediksi} . It has become one of the most popular programming languages worldwide. The structure of the language and its object-oriented approach help programmers to write logical and clear code for small and large projects. Python libraries (packages) effectively simplify many important processes such as analysing and visualizing data, retrieving unstructured data from the web, image processing, building machine learning models, and textual information \cite{Samira_Gholizadeh2022}. Here, it has been used to implement the PSO algorithm to optimize query performance, connects to the database using libraries like pyodbc, measures, executes, and analyzes the database programmatically.

\item\textbf{pyodbc:} Pyodbc is an open-source Python module that makes accessing ODBC\footnote{Open Database Connectivity (ODBC) is a standardized application programming interface (API) for accessing databases.} databases simple. It implements the DB API\footnote{An Application Programming Interface (API) is a set of protocols, tools, and definitions that allow different software applications to communicate with each other.} 2.0 specification but is packed with even more Pythonic convenience. The pyodbc library connects Python to MSSQL, allowing programmatic execution of SQL queries and retrieval of results.

\item\textbf{pandas:} The pandas constitute an open-source data manipulation and analysis tool that is fast, powerful, flexible, and easy to use. It provides data structures like DataFrames and Series, which are particularly useful for working with structured data. It is built entirely on the Python programming language. Pandas is used to analyze query performance data, such as execution times and storage costs, for better insights.

\item\textbf{matplotlib:} Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. \enquote{Matplotlib makes easy things easy and hard things possible} \cite{matplotlib}. It has been used to visualize query performance metrics and PSO convergence, making results easier to interpret.

\item\textbf{Git:} Git is a version control system that is used for tracking the changes made in the files and for enabling a collaborative software world. Git is incredibly popular due to its flexibility, speed, and ability to support almost any workflow. It has become the go-to standard for version control, with around 90 per cent of developers worldwide using it as their primary system. Git is used to track changes in Python scripts, LaTeX documents, and SQL files, making it an essential tool for collaboration and version control during thesis development.

\end{enumerate}

\clearpage

\subsection{Practical Implementation }
 \begin{center}
     \textbf{..... Running ...........}
 \end{center}
\subsubsection{Database Creation}
Choosing a database or creating one if not available is the initial step of the implementation. For the demonstration purpose, one database called \texttt{AccessAuditDB} is created. The database was populated with tables and sample data to simulate a real-world environment. Relationships between tables are defined using primary keys and foreign keys to ensure data integrity and enable efficient querying, enabling the testing and optimization of queries using materialized views and the PSO algorithm.

The following SQL query checks if the database \texttt{AccessAuditDB} exists and creates it if it does not: \vspace{.4cm}

\input{SQL/Create_database}

\subsubsection{Identify Complex Queries} This is a very important part of identifying the frequent queries that need to be optimized. It refers to the process of analyzing a database workload to pinpoint queries that are resource-intensive, frequently executed, or critical to performance. SQL Server Management Studio or direct SQL query can be used to analyze execution logs and identify the frequently executed and resource-intensive queries. Also identify frequently used sub-queries, aggregation (Queries with GROUP BY, SUM, AVG)s, or frequently joins between tables. For example, the following query to identify long-running queries: \vspace{.4cm}

\input{SQL/Query_identify}

The above SQL query retrieves the top 10 most time-consuming queries from SQL Server by analyzing the \(\texttt{sys.dm\_exec\_query\_stats}\) Dynamic Management Views(DMV)\footnote{DMVs are system defined views for database administrators and developers to troubleshoot performance issues, identify missing indexes, analyze query performance, and monitor resource usage efficiently.}. It calculates the total elapsed time in milliseconds, counts the number of executions, and fetches the query text using \(\texttt{sys.dm\_exec\_sql\_text}\). The results are sorted by \(\texttt{total\_elapsed\_time}\) in descending order to highlight the slowest queries for performance tuning and optimization. These queries were then used to create materialized views, which were optimized using the PSO algorithm to improve overall query performance.

\subsubsection{ Materialized views Creation (Indexed Views):}\label{Query_decomposition} Once the top slow queries are sorted out, MVs(In MSSQL Server Mvs are implemented as Indexed views ) are created for each of these queries to store their precomputed results.\vspace{.4cm}

  %\input{SQL/Create_MSSQL}
  \input{SQL/Create_Views}

  


\subsubsection{Automation with the PSO algorithm:} \label{Cost_evaluation}
 The following Python code illustrates the implementation of the PSO algorithm for multiple values. It evaluates various combinations of these parameters and selects the one that yields the best query response time.\vspace{.4cm}
 



\section*{\textbf{Code example to selecting the optimal view using PSO algorithm.} \vspace{.4cm}}


  \input{Python/PSO_Best_Response} \vspace{.4cm}



The Particle Swarm Optimization (PSO) algorithm is implemented to optimize the selection of materialized views in a database system. The goal is to minimize query execution time and CPU cost by selecting the most beneficial views to materialize. The following sections provide a detailed explanation of each function in the implementation.

% Define block styles




% Define colors
% Define custom colors
\definecolor{userColor}{RGB}{173, 216, 230}  % Light blue for User
\definecolor{mainColor}{RGB}{144, 238, 144}  % Light green for Main Function
\definecolor{dbColor}{RGB}{255, 182, 193}    % Light pink for SQL Server
\definecolor{psoColor}{RGB}{255, 223, 186}   % Light orange for PSO Algorithm


\begin{center}
\resizebox{0.95\textwidth}{!}{ % Ensures it stays within page width
\begin{sequencediagram}

    % Participants with colors
    \newinst[1]{User}{\colorbox{userColor}{User}}
    \newinst[2]{Main}{\colorbox{mainColor}{Main Function}}
    \newinst[2]{DB}{\colorbox{dbColor}{SQL Server}}
    \newinst[2]{PSO}{\colorbox{psoColor}{PSO Algorithm}}

    % Sequence Flow
    \begin{call}{User}{Run Script}{Main}{}
        
        % Step 1: Create Connection
        \begin{call}{Main}{Create Connection}{DB}{}
            \mess[0]{DB}{Connection Established}{Main}[above, near end, text=blue]
        \end{call}

        % Step 2: Initialize Queries
        \begin{call}{Main}{Initialize Queries}{DB}{}
            \mess[0]{DB}{Queries Ready}{Main}[above, near end, text=blue]
        \end{call}

        % Step 3: Start PSO Optimization
        \begin{call}{Main}{Start PSO Optimization}{PSO}{}
            
            % Step 3.1: Evaluate Cost Function
            \begin{call}{PSO}{Evaluate Cost Function}{DB}{}
                \mess[0]{DB}{Query Execution Time}{PSO}[above, near end, text=blue]
                \mess[0]{DB}{CPU Cost}{PSO}[above, near end, text=blue]
            \end{call}

            % Step 3.2: Update Best Positions
            \begin{call}{PSO}{Update Best Positions}{PSO}{}
                \mess[0]{PSO}{Updated Global Best}{PSO}[above, near end, text=blue]
            \end{call}

            % Step 3.3: Optimization Complete
            \mess[0]{PSO}{Optimization Complete}{Main}[above, near end, text=blue]
        \end{call}

        % Step 4: Select Optimal Views
        \begin{call}{Main}{Select Optimal Views}{DB}{}
            \mess[0]{DB}{Optimal Views Selected}{Main}[above, near end, text=blue]
        \end{call}

        % Step 5: Close Connection
        \begin{call}{Main}{Close Connection}{DB}{}
            \mess[0]{DB}{Connection Closed}{Main}[above, near end, text=blue]
        \end{call}

        % Step 6: Output Results
        \mess[0]{Main}{Output Results}{User}[above, near end, text=blue]
    \end{call}

    % Notes and Annotations
    \node[below=1.5cm of User, align=center, text width=5cm] (note1) {
        \textbf{Note 1}: The User initiates the script.\\
        \textbf{Note 2}: The Main Function manages the workflow.
    };

    \node[below=1.5cm of DB, align=center, text width=5cm] (note2) {
        \textbf{Note 3}: SQL Server handles database operations.\\
        \textbf{Note 4}: PSO Algorithm optimizes view selection.
    };

\end{sequencediagram}
} % End of resizebox
\end{center}

\definecolor{userColor}{RGB}{173, 216, 230}  % Light blue
\definecolor{mainColor}{RGB}{144, 238, 144}  % Light green
\definecolor{dbColor}{RGB}{255, 182, 193}    % Light pink
\definecolor{psoColor}{RGB}{255, 223, 186}   % Light orange



\begin{center}
\resizebox{0.85\textwidth}{!}{ % ✅ Ensures the diagram fits within the page
\begin{sequencediagram}

    % Participants with modern formatting
    \newinst[1]{User}{\fcolorbox{black}{userColor}{User}}
    \newinst[2]{Main}{\fcolorbox{black}{mainColor}{Main Function}}
    \newinst[2]{DB}{\fcolorbox{black}{dbColor}{SQL Server}}
    \newinst[2]{PSO}{\fcolorbox{black}{psoColor}{PSO Algorithm}}

    % Execution Flow
    \begin{call}{User}{Run Script}{Main}{}
        
        \begin{call}{Main}{Create Connection}{DB}{}
            \mess[0]{DB}{Connection Established}{Main}[above, near start]
        \end{call}

        \begin{call}{Main}{Initialize Queries}{DB}{}
            \mess[0]{DB}{Queries Ready}{Main}[above, near start]
        \end{call}

        \begin{call}{Main}{Start PSO Optimization}{PSO}{}
            
            \begin{call}{PSO}{Evaluate Cost Function}{DB}{}
                \mess[0]{DB}{Query Execution Time}{PSO}[above, near start]
                \mess[0]{DB}{CPU Cost}{PSO}[above, near start]
            \end{call}

            \begin{call}{PSO}{Update Best Positions}{PSO}{}
                \mess[2]{PSO}{Updated Global Best}{PSO}[below, near start]
            \end{call}

            \mess[0]{PSO}{Optimization Complete}{Main}[above, near start]
        \end{call}

        \begin{call}{Main}{Select Optimal Views}{DB}{}
            \mess[0]{DB}{Optimal Views Selected}{Main}[above, near start]
        \end{call}

        \begin{call}{Main}{Close Connection}{DB}{}
            \mess[0]{DB}{Connection Closed}{Main}[above, near start]
        \end{call}

        \mess[0]{Main}{Output Results}{User}[above, near start]
    \end{call}

\end{sequencediagram}
} % ✅ End of resizebox
\end{center}


\begin{sequencediagram}
    \newthread{main}{Main Function}
    \newinst{pso}{PSO Function}
    \newinst{cost}{Cost Function}
    \newinst{db}{Database}

    \begin{call}{main}{Initialize PSO}{pso}{}
    \end{call}

    \begin{call}{pso}{Evaluate Cost}{cost}{}
        \begin{call}{cost}{Execute Query}{db}{Query Results}
        \end{call}
    \end{call}

    \begin{call}{pso}{Update Particles}{pso}{}
    \end{call}

    \begin{call}{pso}{Check Stopping Condition}{pso}{}
    \end{call}

    \begin{call}{pso}{Return Results}{main}{Optimal Views}
    \end{call}
\end{sequencediagram}


\section*{Function Explanations}

\subsection*{create\_connection}
The \texttt{create\_connection} function establishes a connection to the SQL Server database using the provided connection parameters. It uses the \texttt{pyodbc} library to connect to the database and logs the connection status. If the connection fails, an error message is logged, and the function returns \texttt{None}. This function ensures that the database connection is properly initialized before any queries are executed.

\subsection*{cost\_function}
The \texttt{cost\_function} evaluates the cost of selecting a specific set of materialized views. It calculates the total query execution time and CPU cost for the selected views. The CPU cost is estimated based on the execution time and the complexity of the queries, measured by the number of joins. If no views are selected, the function returns a high cost to discourage empty selections. The total cost is computed as a weighted sum of execution time and CPU cost, prioritizing execution time with a weight of 0.7 and CPU cost with a weight of 0.3.

\subsection*{pso}
The \texttt{pso} function implements the Particle Swarm Optimization algorithm with simulated annealing. It initializes a swarm of particles, each representing a potential solution (a set of materialized views). The algorithm iteratively updates the particles' positions and velocities based on their personal best positions and the global best position. Simulated annealing is used to escape local optima by allowing occasional acceptance of worse solutions. The function returns the global best position, representing the optimal set of materialized views, and the corresponding cost.

\subsection*{main}
The \texttt{main} function orchestrates the execution of the PSO algorithm. It establishes a database connection, initializes the list of queries, and sets the PSO parameters (number of particles and iterations). The \texttt{pso} function is called to find the optimal materialized views, and the results are logged. Finally, the database connection is closed. This function serves as the entry point for the program and ensures that all components are executed in the correct order.



\textbf{Output from PSO automation }  The PSO algorithm is implemented to solve the optimization problem. Upon execution, the algorithm produced the following output: \vspace{.4cm}



  \input{Python/Python_output} \vspace{.4cm}
  

  \input{Python/python_output2} \vspace{.4cm}





 For each particle, the PSO algorithm evaluates the cost, updates personal and global best, and updates velocity and position. Finally, it prints the optimal combination of materialized views and the corresponding query response time.
 [0, 1, 0] means the second materialized view is selected for optimal query performance.\vspace{.4cm}
  

\textbf{One more example are given here } \hyperref[test:Test2]{\textcolor{blue}{Test 2}}. \vspace{.4cm}

%\subsubsection{Materialized View Management \& Selection Approach}
\subsubsection{View Maintenance and Refresh Strategies:}\label{View_maintainance} Manual or automatic refresh strategies can be set up according to the requirements query to create a scheduled job: \vspace{.4cm}

\input{SQL/Refresh_config}

\subsubsection{Performance Testing and Data Analysis} After creating the materialized views, performance metrics were collected using SQL query statistics. Query execution times and CPU usages are recorded using built-in database monitoring tools, such as Dynamic Management Views (DMVs) or with the help of SQL queries like. 

 \input{SQL/Query_statistics}

\begin{enumerate}


    \item \textbf{ Query response time comparison:}\\
The tests were performed using \textit{MSSQL} on a system running \textit{Windows 10 Pro} with the following specifications:
\begin{itemize}
    \item \textbf{System Type}: 64-bit operating system, x64-based processor.
    \item \textbf{Processor}: 12th Gen Intel(R) Core(TM) i7-1255U, 1.70 GHz.
    \item \textbf{RAM}: 32 GB.
\end{itemize}\vspace{.4cm}
After executing each query several times, both with and without materialized views, the \textit{response time}\footnote{Time taken to execute queries} and \textit{CPU usage}\footnote{Percentage of CPU utilization during query execution and refresh processes.} were noted in the "Messages" tab in SSMS. The effectiveness of the materialized views was also evaluated using a comparison table and the percentage difference formula.

\begin{itemize}
\item\textbf{Differences between optimization methods:} The \hyperref[comparison_table]{table~\ref*{comparison_table}} presents a performance comparison of query execution times using three different approaches: \textit{Direct SELECT Aggregation}, \textit{Indexed View }, and \textit{Materialized View}. The results show that the MV approach consistently outperforms the other methods, with an average execution time of \textbf{5 ms}, compared to \textbf{14.8 ms} for indexed View query and \textbf{66.6 ms} for direct \textit{SELECT} aggregation. This demonstrates that materialized views significantly reduce query execution times, providing a \textbf{92.5\%} improvement over direct aggregation and a \textbf{66.2\%} improvement over indexed views. The consistent performance across multiple runs highlights the reliability and efficiency of materialized views in optimizing database queries.\vspace{.4cm}
 
 \input{Table/Response_time_comparion}\vspace{.4cm
 }

 \item\textbf{Analysis of percentage differences in query performance:}
 
\input{Math/Differences}

\begin{comment}\[
\text{Difference (\%)} = \frac{\text{Without MV} - \text{With MV}}{\text{Without MV}} \times 100 = \frac{2.35 - 0.45}{2.35} \times 100 \approx 80.85\%
\]
\end{comment}

The output indicates an 80.85\% improvement in performance. Here, \( W = 2.35 \) represents the initial execution time (without optimization), and \( M = 0.45 \) represents the improved execution time (with optimization). This significant reduction in execution time demonstrates the effectiveness of the optimization technique, as it reduces the query processing time by approximately 80.85\%, leading to faster and more efficient database operations.\vspace{.4cm}

\item \textbf{Impact of MV on query execution time:} The performance of queries with and without materialized views (MV) is compared in \hyperref[tab:performance]{Table~\ref*{tab:performance}}. The table shows the execution times for three queries without materialized views and with materialized views, along with the percentage difference in performance. As evident from the results, the use of materialized views significantly reduces query execution time, with improvements ranging from **79.34\%** to **83.61\%**.

\begin{table}[h!]
    \centering
    \caption{Performance Comparison}
    \label{tab:performance}
    \rowcolors{2}{gray!10}{white} % Alternate row colors
    \begin{tabular}{lccc}
        \toprule
        \rowcolor{blue!10} % Header row color
        \textbf{Query} & \textbf{Without MV (s)} & \textbf{With MV (s)} & \textbf{Difference (\%)} \\
        \midrule
        Query 1 & 2.35 & 0.45 & \cellcolor{white!20}80.85 \\
        Query 2 & 3.78 & 0.62 & \cellcolor{gray!10}83.61 \\
        Query 3 & 4.21 & 0.87 & \cellcolor{white!20}79.34 \\
        \bottomrule
    \end{tabular}
\end{table}

%\item \textbf{Bar Chart Comparison:} As shown in the \hyperref[fig:execution-plan]{barchart}, the execution time demonstrates the optimization achieved by using materialized views. Direct select aggregation is the least efficient as it involves computing aggregation directly from the raw data during query execution, which is resource-intensive and time-consuming. On the other hand, a materialized view is the fastest one. Although the choice of method depends on the trade-off between query speed, storage requirements, and maintenance.



%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth]{Figure/Bar_chart.png} % Replace with your image file name
%\caption{Comparison of query execution time} % Caption for the screenshot
%\label{fig:execution-plan} % Label for referencing
%\end{figure}


\end{itemize} \vspace{.4cm}

    \item \textbf{CPU performance analysis:}

The following \hyperref[fig:cpu_usage]{bar chart} illustrates the average CPU usage for query execution before and after materialized views (MVs) are implemented. The x-axis is used to represent two conditions: \textit{Before} MVs (without materialized views) and \textit{After} MVs (with materialized views). The y-axis is used to represent the average CPU usage as a percentage. As it is evident from the graph, the CPU utilization average went down significantly from \textbf{85\%} (before MVs) to \textbf{30\%} (after MVs), showcasing the effectiveness of materialized views in reducing CPU burden in query processing. The reduction can be accounted for due to precomputation and caching of query output, which reduces real-time computation and subsequently CPU consumption.

%\input{Table_chart/cpu_usage}

\begin{figure}[h!] % Use the figure environment
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        ybar, 
        symbolic x coords={Before MV, After MV}, % Labels for x-axis
        xtick=data, 
        ylabel={CPU Usage (\%)}, % Y-axis Label (updated to CPU Usage)
        xlabel={Query Execution Type}, % X-axis Label
        nodes near coords, % Shows values above bars
        bar width=25pt, % Adjusted bar width
        enlarge x limits=0.5, % Spacing between bars
        ymin=0, ymax=100, % Y-axis limits
        area legend, % Enables legend area
        legend style={
            at={(0.5,-0.25)}, % Position of the legend (below the chart)
            anchor=north, % Anchors the legend to the north (top) of the point
            yshift=-5pt, % Adds a small gap between xlabel and legend
            font=\small, % Smaller font for legend
        }, 
        bar shift=0pt, % Aligns bars properly
        ymajorgrids=true, % Adds horizontal grid lines
        grid style={dashed, gray!30}, % Grid line style
        ytick={0, 20, 40, 60, 80, 100}, % Custom Y-axis ticks
        xticklabel style={font=\small}, % Smaller font for x-axis labels
        yticklabel style={font=\small}, % Smaller font for y-axis labels
        axis on top, % Places axis lines on top of the bars
    ]
    \addplot[
        fill=red!30, % First bar fill color (red)
        draw=red, % First bar border color (red)
    ] coordinates {(Before MV, 85)(After MV, 30)}; % First bar value (Before MV)
   % \addlegendentry{Before MV} % Legend entry for the first bar

    \addplot[
        fill=green!30, % Second bar fill color (green)
        draw=green, % Second bar border color (green)
    ] coordinates {(After MV, 30)(After MV, 30)}; % Second bar value (After MV)
    %\addlegendentry{After MV} % Legend entry for the second bar

    \end{axis}
\end{tikzpicture}
\caption{Comparison of CPU Usage}
\label{fig:cpu_usage}
\end{center}
\end{figure}
\end{enumerate}


 \textbf{Data will be updated once the final implementation is done}.

% Create a data file for the heatmap
\begin{filecontents*}{heatmap_data.csv}
Query Type, Before MV, After MV
Query 1, 85, 30
Query 2, 75, 25
Query 3, 90, 35
Query 4, 80, 20
Query 5, 70, 15
\end{filecontents*}

\begin{filecontents*}{heatmap_data.csv}
Query Type, Before MV, After MV
1, 85, 30
2, 75, 25
3, 90, 35
4, 80, 20
5, 70, 15
\end{filecontents*}

% Create a data file for the heatmap
\begin{filecontents*}{heatmap_data.csv}
Query Type, Before MV, After MV
1, 85, 30
2, 75, 25
3, 90, 35
4, 80, 20
5, 70, 15
\end{filecontents*}

% Create a data file for the heatmap
\begin{filecontents*}{heatmap_data.csv}
Query Type, Before MV, After MV
1, 85, 30
2, 75, 25
3, 90, 35
4, 80, 20
5, 70, 15
\end{filecontents*}

\begin{figure}[h!]
\centering
\begin{tikzpicture}
    \begin{axis}[
        colormap={cool}{rgb255=(0,0,255) rgb255=(0,255,255) rgb255=(0,255,0) rgb255=(255,255,0) rgb255=(255,0,0)}, % Custom colormap (blue to red)
        colorbar, % Adds a colorbar
        colorbar style={
            ylabel={CPU Usage (\%)}, % Label for the colorbar
            ytick={0, 20, 40, 60, 80, 100}, % Ticks for the colorbar
        },
        xlabel={Condition}, % X-axis label
        ylabel={Query Type}, % Y-axis label
        xtick={1, 2}, % X-axis ticks
        xticklabels={Before MV, After MV}, % X-axis labels
        ytick={1, 2, 3, 4, 5}, % Y-axis ticks
        yticklabels={Query 1, Query 2, Query 3, Query 4, Query 5}, % Y-axis labels
        enlargelimits=false, % Prevents axis limits from expanding
        axis on top, % Places axis lines on top of the heatmap
        point meta min=0, point meta max=100, % Sets the range for the colormap
    ]
        \addplot[
            matrix plot*, % Creates a heatmap
            mesh/cols=2, % Number of columns in the data
            mesh/rows=5, % Number of rows in the data
            shader=flat corner, % Flat shading for the heatmap
        ] table[
            x=Condition, % X-axis data
            y=Query Type, % Y-axis data
            meta=CPU Usage, % Data for the colormap
            col sep=comma, % Separator in the data file
        ] {heatmap_data.csv};
    \end{axis}
\end{tikzpicture}
\caption{Heatmap of CPU Usage Before and After Materialized Views}
\label{fig:heatmap}
\end{figure}

