%\chapter{Introduction and Motivation}
\section{Introduction}
\subsection{Motivation} The motivation for this thesis is driven by the urgent need to enhance query performance in database system, which are at the heart of many enterprise-level and cloud-based applications. SQL queries are essential for data manipulation and analysis, but they can also be a source of frustration and inefficiency if they are not optimized properly. Despite significant advancements in database technologies, enterprises still face issues such as high latency, poor load balancing, and inefficient data retrieval mechanisms, especially under complex query loads. Addressing these challenges not only improves user experience but also boosts the overall efficiency of data-driven decision-making processes.\\
The collaboration with opta data Gruppe, a company leverages over 50 years of experience in the healthcare sector to provide tailored digital, financial and operational solutions in health providers and organizations, provides a unique opportunity to tackle these challenges in a real-world context. Opta data Gruppe has extensive expertise in managing large-scale databases and offers valuable insights and access to proprietary datasets and infrastructure. This partnership will enable the practical application of theoretical concepts and the evaluation of proposed optimizations in a live environment, ensuring that the research outcomes are both scientifically robust and industrially relevant.\\
By leveraging opta data Gruppe resources and industry experience, this thesis aims to develop a deep understanding of the limitations present in current query optimization strategies used in database systems. Design and test innovative approaches (a sophisticated algorithm solution) capable of overcoming the prevalent hurdles of database. We aim to significantly improve search capabilities that involves the fusion of selection, crossover and mutations operators from PSO algorithm to formulate decision, which can swiftly deduce the most efficient execution plans for the queries, that can predict and adapt to changing data patterns and query demands in real-time. Measure the impacts of these optimizations on query performance, including reduced latency and increased throughput, in an operational setting.\\

%The ultimate goal is to create a set of robust, dynamic query optimization techniques that can be implemented in various types of distributed databases, helping  to manage their data more effectively and gain a competitive edge in the marketplace. This research not only seeks to push the boundaries of academic knowledge but also aims to provide tangible improvements in technology that can be adopted by industry leaders like Opta Data Group.\\



%%Recognizing the limitations of traditional query optimizers in distributed settings, this research is driven by the ambition to develop a sophisticated algorithmic solution capable of overcoming the prevalent hurdles of distributed databases. By proposing a unique optimizer architecture and deploying the Iterative Dichotomizer 3 (ID3) algorithm as a query optimizer, we aim to significantly improve search capability. This involves the fusion of selection, crossover, and mutation operators from genetic algorithms to formulate decision trees, which can swiftly deduce the most efficient execution plans for queries.

%Moreover, the thesis aims to innovate beyond conventional methods by introducing a caching strategy that mitigates the time cost associated with processing numerous queries. Through rigorous testing and comparative experiments, the research evaluates the execution costs and convergence speeds of Top-k query plans, highlighting the effectiveness of the proposed solutions in achieving superior query efficiency, albeit with a trade-off in execution time.

%The thesis also ventures into the realm of artificial intelligence by presenting "Neo" (Neural Optimizer), a groundbreaking learning-based query optimizer that capitalizes on deep neural networks. This innovative approach to generating query execution plans represents a leap forward in query optimization, underscoring the transformative potential of machine learning in the field of database management.






\subsection{Introduction}
The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago.\cite{Blakeley1986EfficientlyUM} The presence of the right materialized views can significantly improve performance, particularly for decision support application. However, to realize this potential, a judicious selection of materialized views is crucial.\cite{agrawal2000automated}\\
A database management system(DBMS) is a crucial software component that enables efficient creation, updating, deleting, and retrieving of data stored in databases. In the era of big data, databases have become a cornerstone for handling vast amounts of information across multiple locations. As the backbone of large-scale online applications, such as social media platforms, e-commerce sites, and cloud services, the ability of these systems to efficiently process queries is paramount. The performance of databases directly influences the responsiveness of applications and by extension, user experience and business operations.\cite{4}\\
Especially in database systems, one of the most important factors related to large databases is query  optimization and response time, on-time access to information and it is the basic requirement of successful business application. A data warehouse implements many materialized views to efficiently process a predefined set of queries with speed. For any database, quick response time and accuracy are very important factors in considering its success.\cite{karde2010selection} 

A necessary condition for the success of a data warehouse is to provide accurate and timely consolidated information for the decision makers, along with fast query response times. For this purpose, a common method used in practice is the use of higher information and the best concept of response time, whereby a query gets answered quicker. One of the most important decisions in designing data Warehouse is selecting views to materialize for the purpose of efficiently supporting the decision making. The view selection problem defined is to select a set of derived views to materialize that minimizes the sum of total query response time  and  maintenance of the selected views. Thus, the goal is to select a good set of views that minimizes the overall query response time and also maintains the selected views. The decision "what is the best set of views to materialize? " is to be made based on the system workload, a sequence of queries and updates that exemplifies the typical load on the system. The criterion can be very simple, for example, one that simply minimizes the overall execution time of workload queries.\\
In relational databases, a view is a function from a set of base tables to a derived table; the function is recomputed every time the view is referenced. A materialized view, on the other hand, is, so to say, a cache: that is, a duplicate of the data represented that can be accessed fast. Therefore, it is evident that the use of materialized views incorporating not just traditional simple SELECT PROJECT JOIN operators but also complex online analytical processing operator contributes significantly to improving online analytical process (OLAP) query performance. Materialized views are used in data warehousing, replication servers, recording systems, and data visualization and mobile systems [2, 3, 4]. In some cases, it can be more advantageous to materialize the view than to have to compute the base tables each time the view is queried. Each time a change is made to the base tables to which the view refers, the materialized view is refreshed. It can be quite costly to rematerialized this view every time a change may affect one of the base tables. So, it is ideal to propagate the changes incrementally; the materialized view should be refreshed for incremental changes to base tables.\cite{Data_warehousing,efficient_incremental,rashid2009role}\\
While the applications of databases are extensive, it presents many unique challenges in query performance optimization. Such systems need to handle not only large volumes of data but also data that is spatially dispersed. Such dispersion requires complex coordination and communication amongst different nodes with inherent latency and potential bottlenecks that can degrade query performances. Moreover, network variability and heterogeneity characteristic features of a database environment further complicate the effective execution of queries.

\subsection{Overview of Opta data Gruppe}
\normalsize

\subsubsection{Opta data Stiftung \& Co. KG }
Opta data Stiftung \& Co. KG acts as the umbrella organization of the opta data Group,
which brings together relevant areas for the entire company. This also includes working student, trainees, although they work and can be trained in various specialist areas. Opta data Holding is a company that specializes in
Billing, IT and services in the healthcare sector.\\ 
\subsubsection{Opta data Finance GmbH  }
Opta data Finance GmbH (referred to as odFIN) is part of opta data Holding and is one of the leading companies in the healthcare billing sector. With a wide range of products, odFIN offers various solutions for service providers and payers in the healthcare sector. As an innovator, the company is actively driving digitalization in the healthcare sector and occupies a leading position in the telemetries infrastructure.\\ 

\subsubsection{Business area egeko  }
The egeko division at odFIN develops and supports software systems for electronic approval procedures in the healthcare sector. With a focus on electronic cost estimates (eKV), the egeko division offers innovative solutions with the software of the same name for service providers in the medical aids and care sector, among others. The company facilitates processes between service providers and health insurance companies by supporting the entire service provision process.\\
The division's software development is made up of a total of three scrum teams and two additional Kanban teams. The scrum teams focus on development with a focus on central services, aids, care and master data \& patient transport. DevOps and quality assurance provide support. 

As a part of Egeko, team DevOps is a cross-functional group that combines software development (Dev) and IT operations (Ops) roles, aiming to create a more efficient and integrated approach to building, testing, and releasing software. Key characteristics of a DevOps team include Automate everything, deployment, infrastructure, test, build, scale promote fault tolerance, server monitoring, static code errors do not directly reduce software quality from the customer's point of view. However, error prevention blocks the CI/CD pipelines Agile approach a platform can be migrated anywhere and at any time Further CIPs to make everyday life easier, avoid click tasks Create mutual trust in each other and in old/new technologies. Active knowledge transfer, definition of consulting measures, support and introduce people to new technologies No fear of new technologies, first evaluate then judge. Create shared visions and responsibility, realize wishes. A start-to-finish responsibility, everyone is involved can get involved and strengthen collaboration. 

The egeko software is used by third parties  customers. To ensure that customer requests are fulfilled and faults are rectified, there is a team responsible for customer support. The customer support team has a ticket database that records messages and faults from customers.\\

I finished my internship in the team DevOps in the egeko division. The DevOps team is part of the development of egeko. I have been assigned the task to continue the Optimization of Database infrastructure of Egeko which is given better result for the existing server in opta data Finance GmbH.  I must do the following Task Monitoring database performance, server health check conduct regular performance tuning, and optimize queries for maximum efficiency. Manage and optimize healthcare databases to ensure the availability and reliability of critical organizational [website] actual and future ETL processes to ensure optimization and best practices. Develop and implement data security policies, procedures, and best practices to protect sensitive healthcare information. Because of that as first of couple week I had to study about past work as the beginner. I used most Microsoft SQL server Management Studio (SSMS), VSCode, Grafana (a multi-platform open-source analytics and interactive visualization web application) mRemoteng, and MSSQL as a programming language.

\subsection{Goal of this thesis}
\normalsize
This thesis conducts a thorough review of the literature on cost estimation (Query performance ) within relational databases, a critical component of the query optimization process. For this systematic review to be effective, precise objectives must be established. These principal aims are outlined as follows in the study.\cite{CostEstimation}
\begin{itemize}
  \item To provide an overview of the materialized view, a efficient  query optimizer in databases with the details of the approaches.
  \item To provide a comprehensive and efficient solution.
  \item To find the limitations of the approaches.
  \item To understand the scope for further research which contributes towards building
better query optimizes/Performance..\cite{CostEstimation}
\end{itemize}
\subsection{Structure of the thesis }The following sections  focus on the objectives of the systematic literature. In section 2 we will provide a background of materialized views  in distributed databases. It includes an overview of the query processing, query optimization and cost estimation with that.\\
In section 3  Methodology. ......Conclusion..\\
At the end of the thesis we  will discuss  future work....Continue....









