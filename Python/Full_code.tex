\begin{lstlisting}[style=pythonstyle, caption={Python script to automate optimal view.}, label={lst:fullCode}]


import pyodbc # A python  library for interacting with ODBC databases like MSSQL that helps to manage database connection
import random #It helps to generate random numbers and choice used to initialize particle positions and velocities
import time
import numpy as np
import logging


# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Connection parameters
server = 'T915-TEST-DB' #Server name
database = 'AccessAuditDB' #Database name
driver_name = 'ODBC Driver 17 for SQL Server'  # ODBC  driver from pyodbc.drivers()
# Uncomment and add these if using SQL Server Authentication
# username = 'm.islam'
# password = 'your_password'


# Establish connection
def create_connection():
    try:
        conn = pyodbc.connect(
            f'DRIVER={{{driver_name}}};'
            f'SERVER={server};'
            f'DATABASE={database};'
            'Trusted_Connection=yes;' # Indicates that Windows Authentication used for Authentication
        )
        logging.info("Connection established!")
        logging.info(f"Connected to database: {database} on server: {server}")
        return conn
    except pyodbc.Error as e:
        logging.error("Error connecting to SQL Server:", exc_info=True)
        return None

# Cost function: Measure total query execution time and CPU cost
def cost_function(selected_views, queries, conn):
    total_time = 0
    cpu_cost = 0

    if not any(selected_views):  # No views selected
        return float('inf'), 0, 0  # High cost for no selection

    cursor = conn.cursor()  # Create a new cursor for this function call
    for i, view in enumerate(selected_views):
        if view == 1:  # If view is selected
            # Measure execution time
            start_time = time.time()
            try:
                logging.info(f"Executing query: {queries[i]}")
                cursor.execute(queries[i])
                cursor.fetchall()
                execution_time = time.time() - start_time
                total_time += execution_time

                # Estimate CPU cost (example: execution time * complexity)
                query_complexity = queries[i].upper().count('JOIN') + 1  # Add 1 to avoid zero complexity
                cpu_cost += execution_time * query_complexity

            except pyodbc.Error as e:
                logging.error(f"Error executing query {i}: {queries[i]}", exc_info=True)
                return float('inf'), 0, 0  # High cost for query errors

    # Weighted cost function (prioritize execution time and CPU cost)
    alpha, beta = 0.7, 0.3  # Weights for execution time and CPU cost.
    #alpha = 0.7 means execution time contributes 70% to the total cost.beta = 0.3 means CPU cost contributes 30% to the total cost.
    total_cost = alpha * total_time + beta * cpu_cost
    return total_cost, total_time, cpu_cost

# Advanced PSO Algorithm
def pso(num_particles, num_iterations, num_queries, queries, conn):
    # PSO parameters # Inertia weight Controls the impact of the previous velocity on the current velocity.
    W_max = 0.9  # Maximum inertia weight 
    W_min = 0.4  # Minimum inertia weight
    c1, c2 = 1.5, 1.5  # # Encourages particles to move toward the personal/global best position.

    v_max = 6.0  # Maximum velocity for clamping

    # Initialize particles
    particles = [{'position': [random.choice([0, 1]) for _ in range(num_queries)],
                 'velocity': [random.uniform(-1, 1) for _ in range(num_queries)],
                 'best_position': None,
                 'best_cost': float('inf')} for _ in range(num_particles)]

    global_best_position = None
    global_best_cost = float('inf')
    global_best_time = 0
    global_best_cpu_cost = 0

    # PSO main loop
    for iteration in range(num_iterations):
        logging.info(f"Iteration {iteration + 1} started.")
        
        # Dynamic inertia weight (decreases over time)
        W = W_max - (W_max - W_min) * (iteration / num_iterations)

        for particle in particles:
            cost, execution_time, cpu_cost = cost_function(particle['position'], queries, conn)
            particle['cost'] = cost

            # Update personal best
            if cost < particle['best_cost']:
                particle['best_position'] = particle['position'][:]
                particle['best_cost'] = cost

            # Update global best
            if cost < global_best_cost:
                global_best_position = particle['position'][:]
                global_best_cost = cost
                global_best_time = execution_time
                global_best_cpu_cost = cpu_cost

        # Update velocity and position
        for particle in particles:
            for i in range(num_queries):
                r1, r2 = random.random(), random.random()
                # Update velocity
                particle['velocity'][i] = (W * particle['velocity'][i] +
                                          c1 * r1 * (particle['best_position'][i] - particle['position'][i]) +
                                          c2 * r2 * (global_best_position[i] - particle['position'][i]))
                # Clamp velocity to avoid explosion
                particle['velocity'][i] = max(min(particle['velocity'][i], v_max), -v_max)
                # Update position using sigmoid function
                sigmoid = 1 / (1 + np.exp(-particle['velocity'][i]))
                particle['position'][i] = 1 if random.random() < sigmoid else 0

        # Log iteration results
        logging.info(f"Iteration {iteration + 1}: Best Cost = {global_best_cost:.4f}, Execution Time = {global_best_time:.4f}, CPU Cost = {global_best_cpu_cost:.4f}")

    return global_best_position, global_best_cost, global_best_time, global_best_cpu_cost

# Main function
def main():
    conn = create_connection()
    if not conn:
        return

    # List of queries corresponding to materialized views
    queries = [
        "SELECT * FROM TotalSalesByCustomer",  
        "SELECT * FROM TotalQuantityByProduct",  
        "SELECT * FROM MonthlySales"  
    ]

    # PSO parameters
    num_particles = 5  # Number of particles in the swarm
    num_iterations = 5  #The number of iterations the algorithm will run
    num_queries = len(queries) #The number of queries (materialized views) to optimize.

    # Run PSO
    logging.info("Starting PSO algorithm...")
    logging.info(f"Number of particles: {num_particles}")
    logging.info(f"Number of iterations: {num_iterations}")
    best_position, best_cost, best_time, best_cpu_cost = pso(num_particles, num_iterations, num_queries, queries, conn)

    # Output optimal materialized views
    optimal_views = [queries[i] for i, view in enumerate(best_position) if view == 1]
    logging.info("Optimal Materialized Views:")
    for view in optimal_views:
        logging.info(f"- {view}")
    logging.info(f"Best Execution Time: {best_time:.4f}")
    logging.info(f"Best CPU Cost: {best_cpu_cost:.4f}")

    # Close connection
    conn.close()
    logging.info("Connection closed.") #Ensures the database connection is closed after the script execution.

if __name__ == "__main__":
    main()

\end{lstlisting}