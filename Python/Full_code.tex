\begin{lstlisting}[style=pythonstyle, caption={Python script to automate optimal view.}, label={lst:fullCode}]
import pyodbc  # Library for connecting to SQL Server using ODBC
import random  # Library for generating random numbers (used in PSO)
import time  # Library for measuring execution time
import numpy as np  # Library for numerical operations (used for sigmoid function)
import logging  # Library for logging messages

# Configure logging to display messages with timestamp, log level, and message
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Connection parameters for SQL Server
server = 'T915-TEST-DB'  # Replace with your SQL Server name
database = 'AccessAuditDB'  # Replace with your database name
driver_name = 'ODBC Driver 17 for SQL Server'  # ODBC driver for SQL Server

# Function to establish a connection to the SQL Server database
def create_connection():
    try:
        # Create a connection string and connect to the database
        conn = pyodbc.connect(
            f'DRIVER={{{driver_name}}};'  # Specify the ODBC driver
            f'SERVER={server};'  # Specify the server name
            f'DATABASE={database};'  # Specify the database name
            'Trusted_Connection=yes;'  # Use Windows Authentication
        )
        logging.info("Connection established!")  # Log success message
        logging.info(f"Connected to database: {database} on server: {server}")  # Log connection details
        return conn  # Return the connection object
    except pyodbc.Error as e:
        logging.error("Error connecting to SQL Server:", exc_info=True)  # Log error message
        return None  # Return None if connection fails

# Cost function: Measures total query execution time and CPU cost
def cost_function(selected_views, queries, conn):
    total_time = 0  # Initialize total execution time
    cpu_cost = 0  # Initialize total CPU cost

    # If no views are selected, return a high cost (infinite)
    if not any(selected_views):
        return float('inf'), 0, 0

    cursor = conn.cursor()  # Create a cursor object to execute queries
    for i, view in enumerate(selected_views):
        if view == 1:  # If the view is selected
            # Measure execution time
            start_time = time.time()
            try:
                logging.info(f"Executing query: {queries[i]}")  # Log the query being executed
                cursor.execute(queries[i])  # Execute the query
                cursor.fetchall()  # Fetch all results (to ensure the query runs completely)
                execution_time = time.time() - start_time  # Calculate execution time
                total_time += execution_time  # Add to total execution time

                # Calculate query complexity based on multiple factors
                query = queries[i].upper()  # Convert query to uppercase for case-insensitive matching

                # Factor 1: Number of JOIN operations
                join_count = query.count('JOIN')

                # Factor 2: Number of tables (FROM clauses)
                from_count = query.count('FROM')

                # Factor 3: Number of subqueries or nested queries (SELECT clauses)
                select_count = query.count('SELECT') - 1  # Subtract 1 for the main SELECT

                # Factor 4: Query length (number of words)
                query_length = len(query.split())

                # Assign weights to each factor
                weight_join = 0.4  # JOIN operations are highly complex
                weight_from = 0.3  # Number of tables contributes to complexity
                weight_select = 0.2  # Subqueries add complexity
                weight_length = 0.1  # Longer queries might be more complex

                # Calculate weighted complexity
                query_complexity = (
                    (join_count * weight_join) +
                    (from_count * weight_from) +
                    (select_count * weight_select) +
                    (query_length * weight_length)
                )

                # Ensure minimum complexity is 1
                query_complexity = max(query_complexity, 1)

                # Calculate CPU cost
                cpu_cost += execution_time * query_complexity

            except pyodbc.Error as e:
                logging.error(f"Error executing query {i}: {queries[i]}", exc_info=True)  # Log query execution error
                return float('inf'), 0, 0  # Return high cost for query errors

    # Weighted cost function (prioritize execution time and CPU cost)
    alpha, beta = 0.7, 0.3   #alpha = 0.7 means execution time contributes 70% to the total cost.beta = 0.3 means CPU cost contributes 30% to the total cost.
    total_cost = alpha * total_time + beta * cpu_cost  # Calculate total cost
    return total_cost, total_time, cpu_cost  # Return total cost, execution time, and CPU cost

# Advanced PSO Algorithm
def pso(num_particles, num_iterations, num_queries, queries, conn):
    # PSO parameters
    W_max = 0.9  # Maximum inertia weight
    W_min = 0.4  # Minimum inertia weight
    c1, c2 = 1.5, 1.5  # Cognitive and social parameters
    v_max = 6.0  # Maximum velocity for clamping

    # Initialize particles
    particles = [{
        'position': [random.choice([0, 1]) for _ in range(num_queries)],  # Randomly initialize position (0 or 1)
        'velocity': [random.uniform(-1, 1) for _ in range(num_queries)],  # Randomly initialize velocity
        'best_position': None,  # Initialize best position as None
        'best_cost': float('inf')  # Initialize best cost as infinity
    } for _ in range(num_particles)]

    # Initialize best_position with the initial position
    for particle in particles:
        particle['best_position'] = particle['position'][:]  # Copy initial position

    global_best_position = None  # Initialize global best position
    global_best_cost = float('inf')  # Initialize global best cost as infinity
    global_best_time = 0  # Initialize global best execution time
    global_best_cpu_cost = 0  # Initialize global best CPU cost

    # List to store iteration data
    iteration_data = []

    # PSO main loop
    for iteration in range(num_iterations):
        logging.info(f"Iteration {iteration + 1} started.")  # Log the start of the iteration

        # Dynamic inertia weight (decreases over time)
        W = W_max - (W_max - W_min) * (iteration / num_iterations)

        for particle in particles:
            # Calculate cost, execution time, and CPU cost for the particle's position
            cost, execution_time, cpu_cost = cost_function(particle['position'], queries, conn)
            particle['cost'] = cost  # Store the cost

            # Update personal best position and cost
            if cost < particle['best_cost']:
                particle['best_position'] = particle['position'][:]  # Update best position
                particle['best_cost'] = cost  # Update best cost

            # Update global best position and cost
            if cost < global_best_cost:
                global_best_position = particle['position'][:]  # Update global best position
                global_best_cost = cost  # Update global best cost
                global_best_time = execution_time  # Update global best execution time
                global_best_cpu_cost = cpu_cost  # Update global best CPU cost

        # Update velocity and position for each particle
        for particle in particles:
            for i in range(num_queries):
                r1, r2 = random.random(), random.random()  # Generate random numbers
                # Update velocity
                particle['velocity'][i] = (W * particle['velocity'][i] +
                                          c1 * r1 * (particle['best_position'][i] - particle['position'][i]) +
                                          c2 * r2 * (global_best_position[i] - particle['position'][i]))
                # Clamp velocity to avoid explosion
                particle['velocity'][i] = max(min(particle['velocity'][i], v_max), -v_max)
                # Update position using sigmoid function
                sigmoid = 1 / (1 + np.exp(-particle['velocity'][i]))  # Sigmoid function
                particle['position'][i] = 1 if random.random() < sigmoid else 0  # Update position

        # Log iteration results
        logging.info(f"Iteration {iteration + 1}: Best Cost = {global_best_cost:.4f}, Execution Time = {global_best_time:.4f}, CPU Cost = {global_best_cpu_cost:.4f}")

        # Store iteration data
        iteration_data.append({
            'iteration': iteration + 1,  # Iteration number
            'execution_time': global_best_time,  # Best execution time
            'cpu_cost': global_best_cpu_cost,  # Best CPU cost
            'best_view': [queries[i] for i, view in enumerate(global_best_position) if view == 1]  # Best view(s)
        })

    return global_best_position, global_best_cost, global_best_time, global_best_cpu_cost, iteration_data

# Main function
def main():
    conn = create_connection()  # Establish database connection
    if not conn:
        return  # Exit if connection fails

    # List of queries corresponding to materialized views
    queries = [
        "SELECT * FROM TotalSalesByCustomer",
        "SELECT * FROM TotalQuantityByProduct",
        "SELECT * FROM MonthlySales"
    ]

    # PSO parameters
    num_particles = 5  # Number of particles in the swarm
    num_iterations = 5  # Number of iterations
    num_queries = len(queries)  # Number of queries (materialized views) to optimize

    # Run PSO
    logging.info("Starting PSO algorithm...")  # Log the start of the PSO algorithm
    logging.info(f"Number of particles: {num_particles}")  # Log the number of particles
    logging.info(f"Number of iterations: {num_iterations}")  # Log the number of iterations
    best_position, best_cost, best_time, best_cpu_cost, iteration_data = pso(num_particles, num_iterations, num_queries, queries, conn)

    # Output optimal materialized views
    optimal_views = [queries[i] for i, view in enumerate(best_position) if view == 1]  # Get optimal views
    logging.info("Optimal Materialized Views:")  # Log the optimal views
    for view in optimal_views:
        logging.info(f"- {view}")  # Log each optimal view
    logging.info(f"Best Execution Time: {best_time:.4f}")  # Log the best execution time
    logging.info(f"Best CPU Cost: {best_cpu_cost:.4f}")  # Log the best CPU cost

    # Print iteration data
    print("\nIteration Data:")
    print("Iteration | Execution Time | CPU Cost | Best View")
    print("-" * 50)
    for data in iteration_data:
        print(f"{data['iteration']:9} | {data['execution_time']:.4f}       | {data['cpu_cost']:.4f}   | {', '.join(data['best_view'])}")

    # Print best view, execution time, and CPU cost as a list
    print("\nBest View, Execution Time, and CPU Cost:")
    print(f"Best View: {optimal_views}")
    print(f"Execution Time: {best_time:.4f}")
    print(f"CPU Cost: {best_cpu_cost:.4f}")

    # Close connection
    conn.close()  # Close the database connection
    logging.info("Connection closed.")  # Log the connection closure

if __name__ == "__main__":
    main()  # Run the main function

\end{lstlisting}